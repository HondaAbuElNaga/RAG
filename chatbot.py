import os
import json
from datetime import datetime
import gradio as gr

# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ù…ÙƒØªØ¨Ø§Øª LangChain Ùˆ Chroma
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_chroma import Chroma

# ØªØ­Ù…ÙŠÙ„ Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦Ø© (.env)
from dotenv import load_dotenv
load_dotenv()

# --- Configuration ---
DATA_PATH = r"data"
CHROMA_PATH = r"chroma_db"
COLLECTION_NAME = "example_collection" 

# Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ù…ÙˆØ°Ø¬ Embeddings (Ù…Ø·Ø§Ø¨Ù‚ Ù„Ù…Ø§ ØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ ÙÙŠ ingest)
embeddings_model = OpenAIEmbeddings(model="text-embedding-3-small")

# Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù€ LLM
llm = ChatOpenAI(temperature=0.1, model='gpt-4o-mini')

# Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Chroma
vector_store = Chroma(
    collection_name=COLLECTION_NAME,
    embedding_function=embeddings_model,
    persist_directory=CHROMA_PATH, 
)

# ØªØ­Ø¯ÙŠØ¯ Ø­Ø¬Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø© (Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© Ø§Ù„ØªÙŠ ÙŠØªÙ… ØªØ°ÙƒØ±Ù‡Ø§)
MEMORY_WINDOW_SIZE = 3 

def stream_response(message, history):
    """
    Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ø§Ù„ØªÙŠ ØªØ¹Ø§Ù„Ø¬ Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ØŒ ØªØ¨Ø­Ø« ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§ØªØŒ
    ÙˆØªÙˆÙ„Ø¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø«Ù… ØªØ­ÙØ¸ Ø§Ù„Ø³Ø¬Ù„.
    """
    
    # 1. Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø°Ø§ÙƒØ±Ø© (ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø­Ø¯ÙˆØ¯Ø©)
    limited_history = history[-MEMORY_WINDOW_SIZE:]
    search_query = message 

    # 2. Ø¥Ø¹Ø§Ø¯Ø© ØµÙŠØ§ØºØ© Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¥Ø°Ø§ ÙˆØ¬Ø¯ ØªØ§Ø±ÙŠØ® Ù„Ù„Ù…Ø­Ø§Ø¯Ø«Ø© (Ù…ÙØ¹Ù„Ø© ÙƒÙ…Ø§ Ø·Ù„Ø¨Øª)
    if history:
        print(f"\n--- DEBUG: History found. Using last {len(limited_history)} turns for rephrasing. ---")
        formatted_history = "\n".join([f"User: {turn[0]}\nAssistant: {turn[1]}" for turn in limited_history])
        
        rephrase_prompt = f"""
        Ø¨Ø§Ù„Ù†Ø¸Ø± Ø¥Ù„Ù‰ ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„ØªØ§Ù„ÙŠ (Ø¢Ø®Ø± {len(limited_history)} Ù…Ø­Ø§Ø¯Ø«Ø§Øª)ØŒ ÙˆØ§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø¬Ø¯ÙŠØ¯ Ù…Ù† Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ØŒ 
        Ø£Ø¹Ø¯ ØµÙŠØ§ØºØ© Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø¬Ø¯ÙŠØ¯ Ù„ÙŠÙƒÙˆÙ† "Ø³Ø¤Ø§Ù„Ø§Ù‹ Ù…Ø³ØªÙ‚Ù„Ø§Ù‹ Ø¨Ø°Ø§ØªÙ‡" (standalone question).

        ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©:
        {formatted_history}

        Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø¬Ø¯ÙŠØ¯: {message}

        Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªÙ‚Ù„:
        """
        
        try:
            rephrase_response = llm.invoke(rephrase_prompt)
            search_query = rephrase_response.content.strip()
            print(f"--- DEBUG: Original Query: '{message}' ---")
            print(f"--- DEBUG: Rephrased Query: '{search_query}' ---")
        except Exception as e:
            print(f"--- ERROR in rephrasing: {e} ---")
            search_query = message 
    else:
        print("\n--- DEBUG: No history. Using original query for search. ---")
    
    # 3. Ø§Ù„Ø¨Ø­Ø« ÙˆØ§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ù…Ù† ChromaDB
    print("--- DEBUG: Searching ChromaDB ---")
    results_with_scores = vector_store.similarity_search_with_score(search_query, k=5) 

    if not results_with_scores:
        print("Database found NO results.")
    else:
        for i, (doc, score) in enumerate(results_with_scores):
            print(f"Result {i+1} [Score: {score:.4f}]: {doc.page_content[:100]}...")
    
    # ØªØµÙÙŠØ© Ø§Ù„Ù†ØªØ§Ø¦Ø¬ (Score Filter)
    good_docs = [doc for doc, score in results_with_scores if score < 1.5]
    
    if not good_docs:
        print("DEBUG: No results passed the filter (Score too high or no results).")
    print("--------------------------------------\n")

    knowledge = ""
    retrieved_context_for_log = [] 

    for doc in good_docs:
        knowledge += doc.page_content + "\n\n"
        retrieved_context_for_log.append(doc.page_content) 


    # 4. Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù€ Prompt ÙˆØ§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù€ LLM
    partial_message = ""
    if message is not None:
        rag_prompt = f"""
        "# Ù‡ÙˆÙŠØªÙƒ ÙˆÙ‚Ø¯Ø±Ø§ØªÙƒ",
        "- Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø·Ù„Ø§Ø¨ ÙˆÙ…ØªØ¯Ø±Ø¨ÙŠÙ† Ø§Ù„Ù…Ø¹Ù‡Ø¯ Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠ Ø§Ù„Ù…ØªØ®ØµØµ Ø§Ù„Ø¹Ø§Ù„ÙŠ  Ù„Ù„ØªØ¯Ø±ÙŠØ¨",
        "- Ù…Ù‡Ù…ØªÙƒ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù‡ÙŠ ØªÙ‚Ø¯ÙŠÙ… Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¯Ù‚ÙŠÙ‚Ø© Ø¹Ù† Ø¨Ø±Ø§Ù…Ø¬ Ø§Ù„Ù…Ø¹Ù‡Ø¯ ÙˆØ¯ÙˆØ±Ø§ØªÙ‡ ÙˆØ¯Ø¨Ù„ÙˆÙ…Ø§ØªÙ‡",
        "- Ø¹Ù„ÙŠÙƒ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø±Ø¯ Ø¨Ù†ÙØ³ Ù„ØºØ© Ø§Ø³ØªÙØ³Ø§Ø± Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…",

        Ø§Ø³ØªØ®Ø¯Ù… "ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©" Ø§Ù„ØªØ§Ù„ÙŠ Ùˆ "Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø©" Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ "Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ø®ÙŠØ±".
        
        ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©:
        {limited_history}
        
        Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø© (Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª):
        {knowledge}
        
        Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ø®ÙŠØ±: {message}
        
        Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:
        """

        print("--- PROMPT BEING SENT TO LLM (Final Answer) ---")
        
        # Stream Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¥Ù„Ù‰ ÙˆØ§Ø¬Ù‡Ø© Gradio
        for response in llm.stream(rag_prompt):
            chunk = response.content
            partial_message += chunk
            yield partial_message
        
        # 5. Ø­ÙØ¸ Ø§Ù„Ø³Ø¬Ù„Ø§Øª (Logs) - Ø§Ù„ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ø¬Ø¯ÙŠØ¯ Ø¨Ø§Ù„ØªØ§Ø±ÙŠØ® ÙˆØ§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ø·Ù„Ù‚
        final_answer = partial_message.strip() 

        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "user_query": message,
            "search_query": search_query, # Ø³ÙŠØ­ÙØ¸ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø¹Ø§Ø¯ ØµÙŠØ§ØºØªÙ‡ Ù‡Ù†Ø§
            "chat_history": history, 
            "retrieved_knowledge": retrieved_context_for_log,
            "bot_answer": final_answer
        }

        try:
            # 1. Ù†Ø­Ø¯Ø¯ Ù…Ø¬Ù„Ø¯ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø­Ø§Ù„ÙŠ Ø¨Ø¯Ù‚Ø© (Ù„Ø¶Ù…Ø§Ù† Ù…ÙƒØ§Ù† Ø§Ù„Ø­ÙØ¸)
            current_script_dir = os.path.dirname(os.path.abspath(__file__))
            
            # 2. Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù ÙŠØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ ØªØ§Ø±ÙŠØ® Ø§Ù„ÙŠÙˆÙ… (Ù…Ù„Ù ÙˆØ§Ø­Ø¯ Ù„ÙƒÙ„ ÙŠÙˆÙ…)
            # Ù…Ø«Ø§Ù„: chat_logs_2023-10-25.jsonl
            timestamp_str = datetime.now().strftime("%Y-%m-%d") 
            log_filename = f"chat_logs_{timestamp_str}.jsonl"
            
            # Ø¯Ù…Ø¬ Ø§Ù„Ù…Ø³Ø§Ø±
            abs_path = os.path.join(current_script_dir, log_filename)
            
            print(f"--- DEBUG: Writing log to: {abs_path} ---")

            with open(abs_path, "a", encoding="utf-8") as f:
                f.write(json.dumps(log_entry, ensure_ascii=False) + "\n")
                f.flush()           # Ø¯ÙØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙˆØ±Ø§Ù‹
                os.fsync(f.fileno()) # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙƒØªØ§Ø¨ØªÙ‡Ø§ ÙØ¹Ù„ÙŠØ§Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù‚Ø±Øµ Ø§Ù„ØµÙ„Ø¨
            
            print(f"--- INFO: Chat log saved successfully. File size: {os.path.getsize(abs_path)} bytes ---")
        except Exception as e:
            print(f"--- ERROR: Failed to write to log file: {e} ---")

# --- ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø±Ø³ÙˆÙ…ÙŠØ© ---
print("Starting Gradio Interface...")

demo = gr.ChatInterface(
    fn=stream_response,
    title="ğŸ¤– Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ù…Ø¹Ù‡Ø¯ Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠ Ø§Ù„Ù…ØªØ®ØµØµ Ø§Ù„Ø¹Ø§Ù„ÙŠ (Ù†Ø³Ø®Ø© ØªØ¬Ø±ÙŠØ¨ÙŠØ©)",
    description="""
    Ø£Ù‡Ù„Ø§Ù‹ Ø¨Ùƒ. Ø§Ø³Ø£Ù„Ù†ÙŠ Ø¹Ù† Ø§Ù„Ø¯Ø¨Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©ØŒ Ø§Ù„Ø´Ø±ÙˆØ·ØŒ Ø£Ùˆ Ø£ÙŠ Ø§Ø³ØªÙØ³Ø§Ø± Ø¢Ø®Ø± ÙŠØ®Øµ Ø§Ù„Ù…Ø¹Ù‡Ø¯.
    """,
    examples=[
        "Ù…Ø§ Ù‡Ùˆ Ø¯Ø¨Ù„ÙˆÙ…  Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„ØªÙ…Ø±ÙŠØ¶ØŸ",
        "Ù…Ø§ Ù‡ÙŠ Ø´Ø±ÙˆØ· Ø§Ù„Ù‚Ø¨ÙˆÙ„ØŸ",
    ],
    theme="soft",
    concurrency_limit=10
)


if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7860, share=True)