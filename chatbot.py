from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_chroma import Chroma
import gradio as gr
import json
from datetime import datetime

# import the .env file
from dotenv import load_dotenv
load_dotenv()

# configuration
DATA_PATH = r"data"
CHROMA_PATH = r"chroma_db"
COLLECTION_NAME = "example_collection" 

# (ØªÙ… ØªØ¹Ø¯ÙŠÙ„Ù‡ Ù„ÙŠØ·Ø§Ø¨Ù‚ ingest_database.py)
embeddings_model = OpenAIEmbeddings(model="text-embedding-3-small")

# initiate the model
llm = ChatOpenAI(temperature=0.1, model='gpt-4o-mini')

# connect to the chromadb
vector_store = Chroma(
    collection_name=COLLECTION_NAME,
    embedding_function=embeddings_model,
    persist_directory=CHROMA_PATH, 
)

# call this function for every message added to the chatbot
def stream_response(message, history):
    
    # --- (1. Ø®Ø·ÙˆØ© Ø¬Ø¯ÙŠØ¯Ø©: Ø¥Ø¹Ø§Ø¯Ø© ØµÙŠØ§ØºØ© Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø°Ø§ÙƒØ±Ø©) ---
    search_query = message # (Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ Ù‡Ùˆ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø£ØµÙ„ÙŠ)

    if history: # (Ù‡Ù„ ÙŠÙˆØ¬Ø¯ ØªØ§Ø±ÙŠØ® Ù„Ù„Ù…Ø­Ø§Ø¯Ø«Ø©ØŸ)
        print("\n--- DEBUG: History found. Attempting to rephrase query. ---")
        
        # (ØªÙ†Ø³ÙŠÙ‚ Ø¨Ø³ÙŠØ· Ù„Ù„Ù€ history)
        formatted_history = "\n".join([f"User: {turn[0]}\nAssistant: {turn[1]}" for turn in history])
        
        rephrase_prompt = f"""
        Ø¨Ø§Ù„Ù†Ø¸Ø± Ø¥Ù„Ù‰ ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„ØªØ§Ù„ÙŠ ÙˆØ§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø¬Ø¯ÙŠØ¯ Ù…Ù† Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ØŒ 
        Ø£Ø¹Ø¯ ØµÙŠØ§ØºØ© Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø¬Ø¯ÙŠØ¯ Ù„ÙŠÙƒÙˆÙ† "Ø³Ø¤Ø§Ù„Ø§Ù‹ Ù…Ø³ØªÙ‚Ù„Ø§Ù‹ Ø¨Ø°Ø§ØªÙ‡" (standalone question) ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ù„Ø¨Ø­Ø« ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª.
        Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø¬Ø¯ÙŠØ¯ Ù…Ø³ØªÙ‚Ù„Ø§Ù‹ Ø¨Ø§Ù„ÙØ¹Ù„ØŒ Ø£Ø¹Ø¯Ù‡ ÙƒÙ…Ø§ Ù‡Ùˆ.
        Ù„Ø§ ØªØ¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¤Ø§Ù„ØŒ ÙÙ‚Ø· Ø£Ø¹Ø¯ ØµÙŠØ§ØºØªÙ‡.

        ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©:
        {formatted_history}

        Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø¬Ø¯ÙŠØ¯: {message}

        Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªÙ‚Ù„:
        """
        
        try:
            # (Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø§Ù„Ù€ LLM ÙÙ‚Ø· Ù„Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØµÙŠØ§ØºØ©)
            rephrase_response = llm.invoke(rephrase_prompt)
            search_query = rephrase_response.content.strip()
            print(f"--- DEBUG: Original Query: '{message}' ---")
            print(f"--- DEBUG: Rephrased Query: '{search_query}' ---")
        except Exception as e:
            print(f"--- ERROR in rephrasing: {e} ---")
            search_query = message # (ÙÙŠ Ø­Ø§Ù„Ø© Ø­Ø¯ÙˆØ« Ø®Ø·Ø£ØŒ Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø£ØµÙ„ÙŠ)
    else:
        print("\n--- DEBUG: No history. Using original query for search. ---")
    
    # --- (2. Ø§Ù„Ø¨Ø­Ø« ÙˆØ§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ - Ù†Ø³ØªØ®Ø¯Ù… "search_query" Ø§Ù„Ù…Ø¹Ø§Ø¯ ØµÙŠØ§ØºØªÙ‡) ---
    print("--- DEBUG: Searching ChromaDB ---")
    results_with_scores = vector_store.similarity_search_with_score(search_query, k=5) 

    if not results_with_scores:
        print("Database found NO results.")
    else:
        for i, (doc, score) in enumerate(results_with_scores):
            print(f"Result {i+1} [Score: {score:.4f}]: {doc.page_content[:100]}...")
    
    good_docs = [doc for doc, score in results_with_scores if score < 1.5]
    
    if not good_docs:
        print("DEBUG: No results passed the filter (Score too high or no results).")
    print("--------------------------------------\n")

    knowledge = ""
    retrieved_context_for_log = [] 

    for doc in good_docs:
        knowledge += doc.page_content + "\n\n"
        retrieved_context_for_log.append(doc.page_content) 


    # --- (3. Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù€ Prompt ÙˆØ§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù€ LLM Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©) ---
    if message is not None:

        partial_message = ""

        # (Ù…Ù‡Ù…: Ø§Ù„Ù€ Prompt Ø§Ù„Ø¢Ù† ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ù€ history Ø¨Ø´ÙƒÙ„ ØµØ±ÙŠØ­)
        rag_prompt = f"""
        "# Ù‡ÙˆÙŠØªÙƒ ÙˆÙ‚Ø¯Ø±Ø§ØªÙƒ",
        "- Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø·Ù„Ø§Ø¨ ÙˆÙ…ØªØ¯Ø±Ø¨ÙŠÙ† Ø§Ù„Ù…Ø¹Ù‡Ø¯ Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠ Ø§Ù„Ù…ØªØ®ØµØµ Ø§Ù„Ø¹Ø§Ù„ÙŠ  Ù„Ù„ØªØ¯Ø±ÙŠØ¨",
        "- Ù…Ù‡Ù…ØªÙƒ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù‡ÙŠ ØªÙ‚Ø¯ÙŠÙ… Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¯Ù‚ÙŠÙ‚Ø© Ø¹Ù† Ø¨Ø±Ø§Ù…Ø¬ Ø§Ù„Ù…Ø¹Ù‡Ø¯ ÙˆØ¯ÙˆØ±Ø§ØªÙ‡ ÙˆØ¯Ø¨Ù„ÙˆÙ…Ø§ØªÙ‡",
        "- Ø¹Ù„ÙŠÙƒ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø±Ø¯ Ø¨Ù†ÙØ³ Ù„ØºØ© Ø§Ø³ØªÙØ³Ø§Ø± Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…",

        Ø§Ø³ØªØ®Ø¯Ù… "ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©" Ø§Ù„ØªØ§Ù„ÙŠ Ùˆ "Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø©" Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ "Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ø®ÙŠØ±".
        
        ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©:
        {history}

        Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø© (Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª):
        {knowledge}
        
        Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ø®ÙŠØ±: {message}
        
        Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:
        """

        print("--- PROMPT BEING SENT TO LLM (Final Answer) ---")
        # (Ø·Ø¨Ø§Ø¹Ø© Ø¬Ø²Ø¡ ØµØºÙŠØ± Ù…Ù† Ø§Ù„Ù€ prompt Ù„ØªØ¬Ù†Ø¨ Ø§Ø²Ø¯Ø­Ø§Ù… Ø§Ù„Ù€ terminal)
        print(rag_prompt[:500] + "...") 
        print("----------------------------------")

        # (Stream Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¥Ù„Ù‰ ÙˆØ§Ø¬Ù‡Ø© Gradio)
        for response in llm.stream(rag_prompt):
            partial_message += response.content
            yield partial_message
        
        # --- (4. ÙƒÙˆØ¯ Ø§Ù„ØªØ³Ø¬ÙŠÙ„ - Ù„Ø§ ÙŠØªØºÙŠØ±) ---
        final_answer = partial_message.strip() 

        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "user_query": message,                    # (Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø£ØµÙ„ÙŠ)
            "search_query": search_query,             # (Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…ÙØ¹Ø§Ø¯ ØµÙŠØ§ØºØªÙ‡)
            "chat_history": history,
            "retrieved_knowledge": retrieved_context_for_log,
            "full_prompt": rag_prompt,
            "bot_answer": final_answer
        }

        try:
            with open("chat_logs.jsonl", "a", encoding="utf-8") as f:
                f.write(json.dumps(log_entry, ensure_ascii=False) + "\n")
            print("--- INFO: Chat log saved successfully. ---")
        except Exception as e:
            print(f"--- ERROR: Failed to write to log file: {e} ---")

# --- (5. ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø±Ø³ÙˆÙ…ÙŠØ© - Ù‡Ø°Ø§ Ù‡Ùˆ Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù†Ø§Ù‚Øµ) ---
print("Starting Gradio Interface...")

demo = gr.ChatInterface(
    fn=stream_response,
    title="ğŸ¤– Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ù…Ø¹Ù‡Ø¯ Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠ Ø§Ù„Ù…ØªØ®ØµØµ Ø§Ù„Ø¹Ø§Ù„ÙŠ",
    description="""
    Ø£Ù‡Ù„Ø§Ù‹ Ø¨Ùƒ. Ø§Ø³Ø£Ù„Ù†ÙŠ Ø¹Ù† Ø§Ù„Ø¯Ø¨Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©ØŒ Ø§Ù„Ø´Ø±ÙˆØ·ØŒ Ø§Ù„Ø±Ø³ÙˆÙ…ØŒ Ø£Ùˆ Ø£ÙŠ Ø§Ø³ØªÙØ³Ø§Ø± Ø¢Ø®Ø± ÙŠØ®Øµ Ø§Ù„Ù…Ø¹Ù‡Ø¯.
    (Ù…Ø«Ø§Ù„: "Ù…Ø§ Ù‡Ùˆ Ø¯Ø¨Ù„ÙˆÙ… Ø§Ù„ØªÙ…Ø±ÙŠØ¶ØŸ" Ø«Ù… "ÙˆÙ…Ø§ Ù‡ÙŠ Ù…Ø¯ØªÙ‡ØŸ")
    """,
    examples=[
        "Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø¯Ø¨Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ø¹ØªÙ…Ø¯Ø©ØŸ",
        "Ù…Ø§ Ù‡Ùˆ Ø¯Ø¨Ù„ÙˆÙ… Ø§Ù„ØªÙ…Ø±ÙŠØ¶ØŸ",
        "ÙƒÙ… Ø±Ø³ÙˆÙ… Ø¯Ø¨Ù„ÙˆÙ… Ø§Ù„Ø­Ø§Ø³Ø¨ Ø§Ù„Ø¢Ù„ÙŠØŸ",
        "Ù…Ø§ Ù‡ÙŠ Ø´Ø±ÙˆØ· Ø§Ù„Ù‚Ø¨ÙˆÙ„ØŸ",
        "Ù‡Ù„ Ø§Ù„ØªØ³Ø¬ÙŠÙ„ Ù…ØªØ§Ø­ Ø§Ù„Ø¢Ù†ØŸ"
    ],
    theme="soft",
    concurrency_limit=10
)

if __name__ == "__main__":
    demo.launch(share=True) # (ÙŠÙ…ÙƒÙ†Ùƒ ØªØºÙŠÙŠØ± share=True Ø¥Ù„Ù‰ False Ø¥Ø°Ø§ Ø£Ø±Ø¯Øª)